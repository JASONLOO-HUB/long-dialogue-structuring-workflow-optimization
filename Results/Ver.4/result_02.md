# MindWeave: 产品概念与初步架构设计

## 初步产品诊断与技术方案建议
产品的核心是解决“线性对话流”与“结构化知识构建”之间的底层矛盾。当前对话式AI因其一维特性，不适合需要多维网状结构的深度学习场景。

- **核心价值主张**：将散点式的对话重构为非线性的知识图谱，旨在解决深度学习过程中的“路径迷失”与“认知负荷”问题。
- **产品定位**：一个“动态生成的交互式教科书”，而非传统的聊天机器人。
- **MVP核心交互**：
    - **主干与分支分离**：采用基于画布（Canvas）的UI，将主线学习内容（Trunk）与补充性的深度探究（Deep-dive Cards）在视觉上分离。
    - **核心验证点**：确保用户的“支线追问”行为不会干扰主线学习流程，并能无缝回归主线。
- **初步技术架构**：
    - **模型层**：采用“大小模型组合”策略。
        - **主控模型 (如 GPT-4o)**：负责生成顶层逻辑与复杂的知识解构。
        - **支线模型 (如 GPT-4o-mini)**：负责解释通用性概念，以优化响应延迟和成本。
    - **数据层**：利用图数据库（如 Neo4j）将用户的学习路径存储为知识图谱，其中节点代表知识点，边代表逻辑关系（如递进、包含）。

## 用户反馈与初期方案迭代

### 对话层级模型初步构思
产品将采用多层对话模型来物理隔离不同深度的信息流，确保学习节奏的清晰。
- **L1 主干层 (Trunk)**：负责核心教学大纲的推进。
- **L2 钻研层 (Dive-in)**：针对主干中的特定概念进行深度挖掘。
- **L3 补充层 (Tangent)**：处理与主线学习无关的临时性或工具性提问。

### 上下文与记忆管理初步策略
为实现高效的上下文管理和Token优化，产品将采用分层和摘要化的记忆策略。
- **分层记忆**：不同层级的对话上下文将被隔离管理，防止信息污染。
- **摘要后处理 (Summary Post-processing)**：当一个“钻研层”的对话分支结束时，系统将利用小型模型自动生成一段“知识增量总结”，并将其挂载到主干上下文的相应节点。这确保了主干在保持简洁的同时，能够继承并利用支线学习的成果。

## 迭代产品诊断与MVP优化建议

### MVP优化方案细化
为确保产品逻辑的健壮性，MVP阶段将聚焦于以下核心模块和验证点。
- **AI原生模块：状态机驱动的“进度锚点”**：AI的角色将从一个简单的对话者转变为一个状态机，严格管理学习进度。
    - **主干层 (L1)**：AI被严格限定为只讨论大纲内容，并具备拒答无关话题的能力。
    - **钻研层 (L2)**：采用覆盖式抽屉（Overlay Drawer）UI，其系统提示词（System Prompt）会自动代入主干层的当前上下文锚点。
- **核心验证点**：验证“多线程上下文隔离”的流畅性，确保用户在三层切换时的体验显著优于打开多个浏览器标签页。
- **记忆管理升级**：
    - **语义索引**：放弃纯JSON文件存储，转而采用本地轻量级数据库（如 SQLite + Vector插件），将对话历史存为“语义索引”，实现更智能的知识检索。

### 技术落地路线图初期规划
- **多模型路由**：建立一个分级模型调用系统，根据任务的复杂度进行智能路由，以平衡成本、速度和逻辑推理能力。
    - **主干层**：使用能力最强的大型模型，确保教学逻辑的严谨性。
    - **钻研层**：使用中等规模的模型，平衡解析深度与响应速度。
    - **补充层**：使用轻量级的小型模型，以最低成本处理临时性问题。

## 核心产品原则确立：AI驱动的IDE
产品最终被定义为一个专为PC端设计的、沉浸式的“AI驱动的IDE（集成学习环境）”，旨在通过极致的Token管理和智能的语义路由，为系统性学习者提供一个高效、无干扰的工具。

### AI驱动IDE的产品定位与Token工程

#### 双层路由引擎设计
为实现单输入框下的智能分流，系统将内置一个双层路由引擎。
- **意图识别层**：使用一个轻量级模型（如GPT-4o-mini）作为“守门员”，其唯一任务是分析用户输入的Prompt，并为其打上路由标签（如 `{target: "L2", ref_node: "node_id_123"}`），决定该问题应被分配到哪个逻辑层。
- **影子上下文 (Shadow Context)**：在执行路由时，系统会自动抓取当前活跃窗口的最后几轮对话作为“局部记忆”，并结合L1的全局大纲作为“全局指引”，合并成一个高效的上下文包发送给后端模型。

#### 极致Token工程与上下文管理
为实现成本效益最大化和长期记忆，采用以下Token管理策略：
- **上下文面包屑 (Context Breadcrumbs)**：在进行深层追问时，系统不会发送完整的上级对话历史，而是发送一个AI压缩后的“路径摘要”（例如：`[领域:金融] -> [章节:现金流] -> [当前节点:DCF模型]`），确保模型在保持上下文感知的同时，极大地节约Token消耗。
- **本地知识索引 (Local Knowledge Indexing)**：历史对话将被存储为本地向量索引。系统平时保持上下文窗口的极简主义，仅在检测到用户提问涉及“过去学过的内容”时，才通过RAG（检索增强生成）进行检索并注入相关上下文。

## 产品项目文档总结与定义

### MindWeave产品初步定义文档

#### 项目背景与痛点诊断
- **路径迷失 (Path Confusion)**：传统线性AI对话在追问细节时，容易导致主线教学大纲被“淹没”，上下文记忆受到严重污染。
- **认知负荷 (Cognitive Load)**：用户为保持逻辑清晰，不得不在多个对话框或应用间频繁切换，手动管理成本极高。
- **效率低下 (Inefficiency)**：重复的背景信息生成浪费了大量Token，且对话内容无法形成结构化的、可复用的知识资产。

#### 产品逻辑架构：三层分流系统
产品采用类似IDE的多维布局，将对话流智能分发至三个逻辑层级：

| 层级 | 角色 (Role) | 交互逻辑 | 上下文策略 |
| :--- | :--- | :--- | :--- |
| **L1: 教学主干 (Trunk)** | 总导师 / 进度条 | 负责生成和推进核心学习大纲，把控整体节奏。 | **持久化**：锁定大纲与核心逻辑，不受琐碎追问干扰。 |
| **L2: 深度钻研 (Dive-in)** | 专家 / 百科 | 针对主干中的特定概念进行深挖。 | **局部化**：继承L1当前节点的背景，结束后生成摘要回传。 |
| **L3: 碎片补充 (Tangent)** | 助手 / 临时笔记 | 处理与主线无关的跳跃性提问或工具性需求。 | **即时化**：窗口关闭即清理，不污染主路径记忆。 |

#### 核心功能与AI原生模块
- **智能路由引擎 (The Intent Router)**：用户在统一的输入框中提问，AI预处理器（轻量模型）会自动识别意图，并将问题分发至对应的L1, L2或L3层级。
- **动态上下文面包屑 (Context Breadcrumbs)**：在L2/L3层级提问时，系统不发送L1的全量历史，而是发送AI压缩后的“路径轨迹”，实现Token优化与精准上下文的平衡。
- **本地化记忆仓库 (Local Knowledge Vault)**：已完成的学习章节被“冷冻”存入本地JSON+向量索引。当后续学习触发相关概念时，通过RAG唤醒记忆，而非重新生成。

#### UI/UX视觉范式初探
*(注：此为最终确定的方案，已整合后续所有迭代)*
- **核心布局：双窗动态IDE架构 (2-Window IDE)**
    - 采用严格的**7:3**比例双窗布局，无缝支持“主次切换”与“挤压平移”动效。
- **导航系统：顶部缩略图与全景地图**
    - **顶部缩略图 (Minimap)**：屏幕顶部常驻一个“地铁线路图”式的横向缩略图，仅显示4个核心节点，作为轻量化导航。
    - **全景地图 (Atlas)**：可唤起全屏的、基于“正交折线”布局的完整知识地图，用于全局概览和快速跳转。
- **交互范式：无限分裂与语义引导**
    - 支持从L1到Ln的**无限层级钻研 (Infinite Dive-in)**。
    - AI在每次回答后提供**三向引导建议**（平级扩展、深层穿透、逻辑收敛），将用户的被动提问转化为主动的路径选择。

#### 技术落地路线图
- **第一阶段：MVP验证**
    - 构建基于轻量模型的路由分类器，测试其在典型学习场景下的识别准确率。
    - 在PC端实现双窗布局及层级切换的核心交互。
- **第二阶段：Token工程优化**
    - 开发“自动摘要回传”机制，实现支线学习成果向主干的挂载。
    - 集成本地数据库存储方案（如SQLite/IndexedDB）。
- **第三阶段：效能飞轮**
    - 支持从外部源（PDF/网页）导入素材，由AI自动生成初始学习大纲（L1）。
    - 增加“一键导出结构化笔记”功能。

#### 投资人建议
产品的核心护城河不在于社区或功能堆砌，而在于**“数据飞轮”**。当用户在工具内沉淀了大量结构化的学习逻辑和个人理解摘要后，其迁移成本将变得极高，这构成了真正的防御力。